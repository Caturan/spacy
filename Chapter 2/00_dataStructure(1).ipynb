{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structes: Vocab, Lexemes and StringStore "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome back! Now that we have had some real experience using spaCy's objects, its time for we learn more about what's actually going on under spaCy's hood. \n",
    "\n",
    "In this lesson, we will take a look at the shared vocabulary and how spaCy deals with strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* spaCy stores all shared data in a vocabulary, the Vocab.\n",
    "\n",
    "* This includes words, but also the labels schemes for tags and entities. \n",
    "* To save memory, all strings are encoded to hash IDs. If a word occurs more than once, we don't need to save it every time. \n",
    "* Instead, spaCy uses a hash function to generate an ID and stores the string only once in the string store. The string nlp.vocab.strings . \n",
    "* It's a lookup tables that works in both directions. We can look up a string and get its hash, and look up a hash to get its string value. Internally, spaCy only communicates in hash IDs. \n",
    "* Hash IDs can't be reversed, though. If a word is not in the vocabulary, there's no way to get its string. That's why we always need to pass around the shared vocab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Vocab: stores data shared across multiple documents \n",
    "* To save memory, spaCy encodes all strings to hash values. \n",
    "* Strings are only stored once in the StringStore via nlp.vocab.strings\n",
    "* Strings store: lookup table in both directions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.vocab.strings.add(\"coffee\")\n",
    "coffee_hash = nlp.vocab.strings[\"coffee\"]\n",
    "coffee_string = nlp.vocab.strings[coffee_hash]\n",
    "\n",
    "# Hashes can't be reversed-that's why we need to provide the shared vocab \n",
    "\n",
    "# Raises an error if we haven't seen the string before \n",
    "string = nlp.vocab.strings[3197928453018144401]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To get the hash for a string, we can look it up in nlp.vocab.strings. \n",
    "* To get the string representation of a hash, we can look up the hash. \n",
    "* A Doc object also exposes its vocab and strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash values:  3197928453018144401\n",
      "string value: coffee\n"
     ]
    }
   ],
   "source": [
    "# Look up string and hash in nlp.vocab.strings \n",
    "doc = nlp(\"I love coffee\")\n",
    "print(\"hash values: \", nlp.vocab.strings[\"coffee\"])\n",
    "print(\"string value:\", nlp.vocab.strings[3197928453018144401]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash values: 3197928453018144401\n"
     ]
    }
   ],
   "source": [
    "# The doc also exposes the vocab and strings \n",
    "doc = nlp(\"I love coffee\")\n",
    "print(\"hash values:\", doc.vocab.strings[\"coffee\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexems: entries in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lexemes are context-independent entries in the vocabulary. \n",
    "\n",
    "* We can get a lexeme by looking up a string or hash ID in the vocab. \n",
    "* Lexemes expose attributes, just like tokens. \n",
    "* They hold context-independent information about a word, like the text, or whether the word consist of alphabetic characters. \n",
    "* Lexemes don't have part-of-speech tags, dependencies or entity labels. Those depend on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee 3197928453018144401 True\n"
     ]
    }
   ],
   "source": [
    "# A Lexeme object is an entry in the vocabulary \n",
    "doc = nlp(\"I love coffee\")\n",
    "lexeme = nlp.vocab[\"coffee\"]\n",
    "\n",
    "# Print the lexical attributes\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Contains the context-independent information about a word. \n",
    "    * Word text: lexeme.text and lexeme.orth (the hash)\n",
    "    * Lexical attributes like lexeme.is_alpha \n",
    "    * Not context-dependent part-of-speech tags, dependencies or entity labels. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
